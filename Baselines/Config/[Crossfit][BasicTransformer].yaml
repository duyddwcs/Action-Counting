# Default experiment config for signal counting
# Each experiment config is recommended name as [Dataset][Encoder][RegressionHead][Loss][Time]

############################################################
#                  General setting
############################################################

# Training
Train:
  # Training epochs
  Epoch: 25
  # Learning rate
  LR: 1e-4
  # Weight Decay
  WeightDecay: 1e-5
  # Loss, loss is in ['Count', 'Density', 'Combine']
  Loss: Count

Val:
  Adaptation: False

# Model in [BasicTransformer, RepNet, TransRAC]
Model: BasicTransformer

# Dataset
Dataset: CrossfitDataset

# CARA setting
CARA:
  # Path to the split json file
  Train_val_split_path: './CARA/Train_val_split.json'
  # Root dir
  Root_dir: '/home/yifeng/CHI2023'
  # Step size of the slide window
  Step_size: [5]
  # Max length
  Max_len: 6000
  # Window size for each token
  Window_size: [10]
  # If generate the density map
  Density_map: False
  # Length of density map
  Density_map_length: 128
  # Multi scale sampling
  Multi: False

# MMFit setting
Crossfit:
  # Path to the split json file
  Train_val_split_path: './Crossfit/Train_val_split.json'
  # Root dir
  Root_dir: '/home/yifeng/CHI2023'
  # Step size of the slide window
  Step_size: [5]
  # Max length
  Max_len: 500
  # Window size for each token
  Window_size: [10]
  # If generate the density map
  Density_map: False
  # Length of density map
  Density_map_length: 128
  # Multi scale sampling
  Multi: False


# Setting of TransformerEncoder
TransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: [60]
  # Max window num(This has to be the same as the dataset)
  max_len: 500
  # Intermediate dim
  inter_dim: 128
  # Head num of attention
  head_num: 4
  # Layer num of the encoder
  layers_num: 1
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

MultiTransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: [60]
  # Max window num
  max_len: 1600
  # Intermediate dim
  inter_dim: 96
  # Head num of attention
  head_num: 4
  # Layer num of the encoder
  layers_num: 1
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

# A simple MLP with a adaptive pooling layer
AdaptRegressionHead:
  # Length after pooling
  pool_len : 96
  # Dim of the following mlp(Has to be a list)
  mlp_dim : [512, 32]
  # Activation
  act: relu

# GPU ID, since the experiment is small, I only use one GPU
GPUID: 3
# Random seed, to reproduce the result, please don't change the random seed
Seed: 5