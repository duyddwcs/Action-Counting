# Default experiment config for signal counting
# Each experiment config is recommended name as [Dataset][Encoder][RegressionHead][Loss][Time]

############################################################
#                  General setting
############################################################

# Training
Train:
  # Training epochs
  Epoch: 20
  # Learning rate
  LR: 1e-6
  # Weight Decay
  WeightDecay: 1e-5
  # Loss, loss is in ['Count', 'Density', 'Combine']
  Loss: Count

Val:
  Adaptation: False

# Model in [BasicTransformer, RepNet, TransRAC]
Model: TransRAC

# Dataset
Dataset: CaraDataset

# CARA setting
CARA:
  # Path to the split json file
  Train_val_split_path: './CARA/Train_val_split.json'
  # Root dir
  Root_dir: '/home/yifeng/SignalCounting'
  # Step size of the slide window
  Step_size: [5, 10, 15]
  # Max length
  Max_len: 6000
  # Window size for each token
  Window_size: [15, 20, 25]
  # If generate the density map
  Density_map: False
  # Length of density map
  Density_map_length: 128
  # Multi scale sampling
  Multi: False

# Setting of TransformerEncoder
TransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: [90, 120, 150]
  # Max window num(This has to be the same as the dataset)
  max_len: 6000
  # Intermediate dim
  inter_dim: 192
  # Head num of attention
  head_num: 4
  # Layer num of the encoder
  layers_num: 1
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

MultiTransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: 60
  # Max window num
  max_len: 6000
  # Intermediate dim
  inter_dim: 96
  # Head num of attention
  head_num: 4
  # Layer num of the encoder
  layers_num: 1
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

# A simple MLP with a adaptive pooling layer
TransRACRegressionHead:
  # Length after pooling
  pool_len : 96
  # Dim of the following mlp(Has to be a list)
  mlp_dim : [512, 32]
  # Activation
  act: relu
  # Transformer_head_num
  transformer_head: 8
  # Transformer_inter_dim
  transformer_dim: 512
  # Transformer_layer_num
  transformer_layer_num: 1
  # Conv out dim
  conv_out_dim: 4

# GPU ID, since the experiment is small, I only use one GPU
GPUID: 0
# Random seed, to reproduce the result, please don't change the random seed
Seed: 5