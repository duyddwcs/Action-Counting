# Training
Train:
  # Training epochs
  Epoch: 50
  # Learning rate
  LR: 2e-5
  # Weight Decay
  WeightDecay: 1e-5

# Model
Model:
  # Encoder
  Encoder: MultiTransformerEncoder
  # Aggregator
  RegressionHead: MultiAdaptConvRegressionHead

# Dataset
Dataset:
  # Dataset Name
  Name: MultiCaraDataset
  # Window size for each token
  Windowsize: 5

# Setting of TransformerEncoder
TransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: 60
  # Max window num
  max_len: 6000
  # Intermediate dim
  inter_dim: 96
  # Head num of attention
  head_num: 4
  # Layer num of the encoder
  layers_num: 1
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

MultiTransformerEncoder:
  # Feat_dim will not be used, since the input feature dim depends on the window_size
  feat_dim: 60
  # Max window num
  max_len: 6000
  # Intermediate dim
  inter_dim: 128
  # Head num of attention
  head_num: 8
  # Layer num of the encoder
  layers_num: 2
  # Dropout rate
  dropout: 0.1
  # If freeze, drop out is set to zero
  freeze: False
  # Activate function
  activation: gelu

# A simple MLP with a adaptive pooling layer
AdaptRegressionHead:
  # Length after pooling
  pool_len : 96
  # Dim of the following mlp(Has to be a list)
  mlp_dim : [512, 32]

AdaptConvRegressionHead:
  # Length after pooling
  pool_len: 96
  # Dim of the following mlp(Has to be a list)
  mlp_dim: [ 512, 32 ]

MultiAdaptConvRegressionHead:
  # Length after pooling
  pool_len: 128
  # Dim of the following mlp(Has to be a list)
  mlp_dim: [ 512, 32 ]

GPUID: 0

# Final Result
# MAE: 10.762
# RMSE: 12.828